"""
å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
è´Ÿè´£å®šæ—¶æŠ“å–çƒ­æ¦œå¹¶æ¨é€ï¼Œæ”¯æŒåŠ¨æ€é…ç½®å’ŒçŠ¶æ€ç®¡ç†
æ”¯æŒå®šæ—¶æ‘˜è¦åŠŸèƒ½
"""
import json
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from apscheduler.triggers.cron import CronTrigger
from datetime import datetime
from typing import List, Optional, Dict, Any
from app.config import settings
from app.services.rss_fetcher import rss_fetcher
from app.services.push_service import push_service
from app.services.database import db
from app.models.schemas import PushMessage, HotItem
from app.utils.sources import HOT_SOURCES
from app.services.config_service import config_service
from app.utils.logger import logger


# é»˜è®¤æ‘˜è¦é…ç½®
DEFAULT_DIGEST_CONFIG = {
    "enabled": False,
    "time": "08:00",  # æ¯å¤©æ¨é€æ—¶é—´
    "sources": [],  # ç©ºè¡¨ç¤ºæ‰€æœ‰æº
    "top_n": 10,  # æ¯ä¸ªæºå–å‰ N æ¡
    "weekdays": [1, 2, 3, 4, 5, 6, 7]  # æ¯å‘¨å“ªå‡ å¤©æ¨é€ï¼Œ1=å‘¨ä¸€ï¼Œ7=å‘¨æ—¥
}


class SchedulerService:
    """è°ƒåº¦å™¨æœåŠ¡"""

    def __init__(self):
        self.scheduler = AsyncIOScheduler()
        self._is_paused = False
        self._last_run = None
        self._last_run_result = None
        self._last_digest_run = None
        self._last_digest_result = None

    def get_status(self) -> dict:
        """è·å–è°ƒåº¦å™¨çŠ¶æ€"""
        job = self.scheduler.get_job("fetch_and_push")

        # ä»æ•°æ®åº“è¯»å–é…ç½®
        interval = self._get_interval()
        enabled_setting = db.get_setting("scheduler_enabled")
        enabled = enabled_setting != "0" if enabled_setting else True

        return {
            "running": self.scheduler.running and not self._is_paused,
            "paused": self._is_paused,
            "enabled": enabled,
            "interval_minutes": interval,
            "next_run": job.next_run_time.isoformat() if job and job.next_run_time else None,
            "last_run": self._last_run.isoformat() if self._last_run else None,
            "last_run_result": self._last_run_result
        }

    def _get_interval(self) -> int:
        """è·å–æŠ“å–é—´éš”"""
        interval_setting = db.get_setting("fetch_interval")
        if interval_setting:
            try:
                return int(interval_setting)
            except ValueError:
                pass
        return settings.fetch_interval_minutes

    def update_interval(self, minutes: int):
        """æ›´æ–°æŠ“å–é—´éš”"""
        job = self.scheduler.get_job("fetch_and_push")
        if job:
            self.scheduler.reschedule_job(
                "fetch_and_push",
                trigger=IntervalTrigger(minutes=minutes)
            )
            logger.info(f"æŠ“å–é—´éš”å·²æ›´æ–°ä¸º {minutes} åˆ†é’Ÿ")

    def pause(self):
        """æš‚åœè°ƒåº¦å™¨"""
        job = self.scheduler.get_job("fetch_and_push")
        if job:
            job.pause()
            self._is_paused = True
            logger.info("è°ƒåº¦å™¨å·²æš‚åœ")

    def resume(self):
        """æ¢å¤è°ƒåº¦å™¨"""
        job = self.scheduler.get_job("fetch_and_push")
        if job:
            job.resume()
            self._is_paused = False
            logger.info("è°ƒåº¦å™¨å·²æ¢å¤")

    async def trigger_fetch(self):
        """æ‰‹åŠ¨è§¦å‘ä¸€æ¬¡æŠ“å–"""
        await self._fetch_and_push_job()

    # ===== å®šæ—¶æ‘˜è¦ç›¸å…³æ–¹æ³• =====

    def get_digest_config(self) -> Dict[str, Any]:
        """è·å–æ‘˜è¦é…ç½®"""
        config_str = db.get_setting("digest_config")
        if config_str:
            try:
                return json.loads(config_str)
            except json.JSONDecodeError:
                pass
        return DEFAULT_DIGEST_CONFIG.copy()

    def set_digest_config(self, config: Dict[str, Any]):
        """è®¾ç½®æ‘˜è¦é…ç½®"""
        # åˆå¹¶é»˜è®¤é…ç½®
        full_config = DEFAULT_DIGEST_CONFIG.copy()
        full_config.update(config)
        db.set_setting("digest_config", json.dumps(full_config))
        
        # æ›´æ–°å®šæ—¶ä»»åŠ¡
        self._update_digest_job(full_config)
        logger.info(f"æ‘˜è¦é…ç½®å·²æ›´æ–°: {full_config}")

    def get_digest_status(self) -> dict:
        """è·å–æ‘˜è¦ä»»åŠ¡çŠ¶æ€"""
        config = self.get_digest_config()
        job = self.scheduler.get_job("daily_digest")
        
        return {
            "enabled": config.get("enabled", False),
            "time": config.get("time", "08:00"),
            "sources": config.get("sources", []),
            "top_n": config.get("top_n", 10),
            "weekdays": config.get("weekdays", [1, 2, 3, 4, 5, 6, 7]),
            "next_run": job.next_run_time.isoformat() if job and job.next_run_time else None,
            "last_run": self._last_digest_run.isoformat() if self._last_digest_run else None,
            "last_run_result": self._last_digest_result
        }

    def _update_digest_job(self, config: Dict[str, Any]):
        """æ›´æ–°æ‘˜è¦å®šæ—¶ä»»åŠ¡"""
        job_id = "daily_digest"
        
        # å…ˆç§»é™¤æ—§ä»»åŠ¡
        existing_job = self.scheduler.get_job(job_id)
        if existing_job:
            self.scheduler.remove_job(job_id)
        
        if not config.get("enabled", False):
            logger.info("æ‘˜è¦ä»»åŠ¡å·²ç¦ç”¨")
            return
        
        # è§£ææ—¶é—´
        time_str = config.get("time", "08:00")
        try:
            hour, minute = map(int, time_str.split(":"))
        except ValueError:
            hour, minute = 8, 0
        
        # è§£ææ˜ŸæœŸ
        weekdays = config.get("weekdays", [1, 2, 3, 4, 5, 6, 7])
        if not weekdays:
            weekdays = [1, 2, 3, 4, 5, 6, 7]
        
        # APScheduler ä½¿ç”¨ 0-6 è¡¨ç¤ºå‘¨ä¸€åˆ°å‘¨æ—¥
        cron_days = ",".join(str((d - 1) % 7) for d in weekdays)
        
        # æ·»åŠ æ–°ä»»åŠ¡
        self.scheduler.add_job(
            self._digest_job,
            trigger=CronTrigger(hour=hour, minute=minute, day_of_week=cron_days),
            id=job_id,
            name="æ¯æ—¥çƒ­æ¦œæ‘˜è¦",
            replace_existing=True
        )
        logger.info(f"æ‘˜è¦ä»»åŠ¡å·²è®¾ç½®: æ¯å¤© {time_str}ï¼Œæ˜ŸæœŸ {weekdays}")

    async def trigger_digest(self, is_test: bool = True):
        """æ‰‹åŠ¨è§¦å‘ä¸€æ¬¡æ‘˜è¦æ¨é€"""
        await self._digest_job(is_test=is_test)

    async def _digest_job(self, is_test: bool = False):
        """æ‰§è¡Œæ‘˜è¦æ¨é€ä»»åŠ¡
        
        Args:
            is_test: æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼ï¼Œæµ‹è¯•æ¨¡å¼ä¸‹æ¯ä¸ªæºåªå–2æ¡ï¼Œæœ€å¤š20æ¡
        """
        logger.info(f"å¼€å§‹ç”Ÿæˆçƒ­æ¦œæ‘˜è¦...{'ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰' if is_test else ''}")
        self._last_digest_run = datetime.now()
        
        try:
            config = self.get_digest_config()
            
            # å®šæ—¶ä»»åŠ¡æ‰æ£€æŸ¥æ˜ŸæœŸï¼Œæ‰‹åŠ¨è§¦å‘ä¸æ£€æŸ¥
            if not is_test:
                current_weekday = datetime.now().isoweekday()
                weekdays = config.get("weekdays", [1, 2, 3, 4, 5, 6, 7])
                if weekdays and current_weekday not in weekdays:
                    logger.info(f"ä»Šå¤©æ˜¯æ˜ŸæœŸ{current_weekday}ï¼Œä¸åœ¨æ‘˜è¦æ¨é€æ—¥æœŸå†…ï¼Œè·³è¿‡")
                    self._last_digest_result = {"success": True, "skipped": True, "reason": "not_in_weekdays"}
                    return
            
            # æ£€æŸ¥æ¨é€æ¸ é“
            configured_channels = push_service.get_configured_channels()
            if not configured_channels:
                logger.warning("æœªé…ç½®ä»»ä½•æ¨é€æ¸ é“ï¼Œè·³è¿‡æ‘˜è¦æ¨é€")
                self._last_digest_result = {"success": False, "error": "no_channels"}
                return
            
            # è·å–è¦åŒ…å«çš„æº
            source_filter = config.get("sources", [])
            # æµ‹è¯•æ¨¡å¼ï¼šæ¯ä¸ªæºåªå–2æ¡ï¼›æ­£å¼æ¨¡å¼ï¼šæŒ‰é…ç½®
            top_n = 2 if is_test else config.get("top_n", 10)
            
            # æŠ“å–æ‰€æœ‰çƒ­æ¦œ
            hot_lists = await rss_fetcher.fetch_all_hot_lists(source_ids=list(HOT_SOURCES.keys()))
            
            # è¿‡æ»¤æº
            if source_filter:
                hot_lists = [h for h in hot_lists if h.source in source_filter]
            
            if not hot_lists:
                logger.warning("æ²¡æœ‰å¯ç”¨çš„çƒ­æ¦œæ•°æ®")
                self._last_digest_result = {"success": False, "error": "no_data"}
                return
            
            # æ„å»ºæ‘˜è¦æ¶ˆæ¯
            digest_items = []
            for hot_list in hot_lists:
                if hot_list.items:
                    # æ·»åŠ æ¥æºæ ‡é¢˜
                    for idx, item in enumerate(hot_list.items[:top_n]):
                        item_with_source = HotItem(
                            id=f"{hot_list.source}_{idx}",
                            title=f"[{hot_list.source_name}] {item.title}",
                            url=item.url,
                            hot_score=item.hot_score,
                            source=hot_list.source
                        )
                        digest_items.append(item_with_source)
            
            if not digest_items:
                logger.warning("æ‘˜è¦å†…å®¹ä¸ºç©º")
                self._last_digest_result = {"success": False, "error": "empty_digest"}
                return
            
            # æ„å»ºæ¨é€æ¶ˆæ¯
            today = datetime.now().strftime("%mæœˆ%dæ—¥")
            title_suffix = "ï¼ˆæµ‹è¯•ï¼‰" if is_test else ""
            message = PushMessage(
                title=f"ğŸ“° {today} çƒ­æ¦œæ‘˜è¦{title_suffix}",
                content=f"ä»Šæ—¥çƒ­æ¦œæ±‡æ€»ï¼Œå…± {len(hot_lists)} ä¸ªæ¥æº",
                source="digest",
                items=digest_items  # æ‰€æœ‰æºéƒ½æ¨é€ï¼Œæ¡æ•°ç”± top_n æ§åˆ¶
            )
            
            # æ¨é€
            results = await push_service.push_to_all(message)
            
            # è®°å½•å†å²
            success_count = sum(1 for s in results.values() if s)
            for channel, success in results.items():
                db.add_push_history(
                    channel=channel,
                    source="digest",
                    title=message.title,
                    item_count=len(digest_items),
                    status="success" if success else "failed"
                )
            
            self._last_digest_result = {
                "success": True,
                "sources_count": len(hot_lists),
                "items_count": len(digest_items),
                "channels_success": success_count
            }
            logger.info(f"æ‘˜è¦æ¨é€å®Œæˆ: {len(hot_lists)} ä¸ªæºï¼Œ{len(digest_items)} æ¡å†…å®¹")
            
        except Exception as e:
            logger.error(f"æ‘˜è¦ä»»åŠ¡å¤±è´¥: {e}")
            self._last_digest_result = {"success": False, "error": str(e)}

    async def _fetch_and_push_job(self):
        """æŠ“å–çƒ­æ¦œå¹¶æ¨é€æ–°å†…å®¹"""
        logger.info("å¼€å§‹æŠ“å–çƒ­æ¦œ...")
        self._last_run = datetime.now()

        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰é…ç½®æ¨é€æ¸ é“
            configured_channels = push_service.get_configured_channels()
            if not configured_channels:
                logger.warning("æœªé…ç½®ä»»ä½•æ¨é€æ¸ é“ï¼Œè·³è¿‡æ¨é€")

            # è·å–ç”¨æˆ·é€‰æ‹©çš„æ¨é€æ•°æ®æº
            push_source_filter = config_service.get_push_sources()

            # è·å–è‡ªå®šä¹‰æ•°æ®æº
            custom_sources = db.get_all_custom_sources()
            custom_source_ids = [s["id"] for s in custom_sources if s["enabled"]]

            # ç¡®å®šè¦æŠ“å–çš„å†…ç½®æ•°æ®æº
            builtin_source_ids = list(HOT_SOURCES.keys())
            if push_source_filter is not None:
                # ç”¨æˆ·å·²é…ç½®æ•°æ®æºè¿‡æ»¤ï¼ŒåªæŠ“å–é€‰ä¸­çš„å†…ç½®æº
                builtin_source_ids = [s for s in builtin_source_ids if s in push_source_filter]
                logger.info(f"æ¨é€æ•°æ®æºè¿‡æ»¤ï¼šå·²é€‰ä¸­ {len(builtin_source_ids)} ä¸ªå†…ç½®æº")

            # åˆå¹¶å†…ç½®å’Œè‡ªå®šä¹‰æ•°æ®æº
            all_source_ids = builtin_source_ids + custom_source_ids

            # æŠ“å–é€‰ä¸­çš„çƒ­æ¦œ
            hot_lists = await rss_fetcher.fetch_all_hot_lists(source_ids=builtin_source_ids)

            # æŠ“å–è‡ªå®šä¹‰æ•°æ®æºï¼ˆåŒæ ·å—æ¨é€æ•°æ®æºé€‰æ‹©é™åˆ¶ï¼‰
            for custom in custom_sources:
                if custom["enabled"]:
                    # å¦‚æœç”¨æˆ·é…ç½®äº†æ•°æ®æºè¿‡æ»¤ï¼Œæ£€æŸ¥è‡ªå®šä¹‰æºæ˜¯å¦è¢«é€‰ä¸­
                    if push_source_filter is not None and custom["id"] not in push_source_filter:
                        logger.debug(f"è‡ªå®šä¹‰æº {custom['name']} æœªè¢«é€‰ä¸­ï¼Œè·³è¿‡")
                        continue
                    hot_list = await rss_fetcher.fetch_custom_source(custom)
                    if hot_list:
                        hot_lists.append(hot_list)

            # è·å–æ¨é€è§„åˆ™
            rules = db.get_enabled_push_rules()

            total_new = 0
            total_pushed = 0
            
            # æ”¶é›†æ‰€æœ‰æ›´æ–°å†…å®¹ï¼Œç”¨äºåˆå¹¶æ¨é€
            all_updates = []  # [(source_name, source, filtered_items, new_items)]

            for hot_list in hot_lists:
                # æ£€æµ‹æ–°å¢å†…å®¹
                new_items, is_first_fetch = rss_fetcher.get_new_items(hot_list.source, hot_list.items)

                if is_first_fetch:
                    logger.info(f"{hot_list.source_name}: é¦–æ¬¡æŠ“å–ï¼Œå·²ç¼“å­˜ {len(hot_list.items)} æ¡ï¼Œè·³è¿‡æ¨é€")
                    continue

                if new_items and configured_channels:
                    # åº”ç”¨æ¨é€è§„åˆ™è¿‡æ»¤
                    filtered_items = self._apply_rules(new_items, hot_list.source, rules)

                    if filtered_items:
                        logger.info(f"{hot_list.source_name} æœ‰ {len(filtered_items)} æ¡æ–°çƒ­ç‚¹ï¼ˆè¿‡æ»¤åï¼‰")
                        total_new += len(filtered_items)
                        # æ”¶é›†æ›´æ–°ï¼Œæ¯ä¸ªæºæœ€å¤šå–5æ¡ç”¨äºåˆå¹¶æ¨é€
                        all_updates.append((hot_list.source_name, hot_list.source, filtered_items[:5], new_items))
                else:
                    logger.debug(f"{hot_list.source_name}: {len(hot_list.items)} æ¡ï¼Œæ— æ–°å¢")
            
            # åˆå¹¶æ¨é€ï¼šå°†æ‰€æœ‰æ›´æ–°åˆå¹¶æˆä¸€æ¡æ¶ˆæ¯
            if all_updates and configured_channels:
                # æ„å»ºåˆå¹¶åçš„æ¶ˆæ¯
                all_items = []
                source_names = []
                for source_name, source, items, _ in all_updates:
                    source_names.append(source_name)
                    for item in items:
                        # ç»™æ¯ä¸ªæ¡ç›®åŠ ä¸Šæ¥æºæ ‡è¯†
                        all_items.append(HotItem(
                            id=item.id,
                            title=f"[{source_name}] {item.title}",
                            url=item.url,
                            hot_score=item.hot_score,
                            source=source
                        ))
                
                message = PushMessage(
                    title=f"ğŸ”¥ çƒ­æ¦œæ›´æ–°ï¼ˆ{len(all_updates)} ä¸ªå¹³å°ï¼‰",
                    content=f"æ¥æºï¼š{', '.join(source_names)}",
                    source="combined",
                    items=all_items
                )
                
                # æ¨é€åˆ°æ‰€æœ‰æ¸ é“
                results = await push_service.push_to_all(message)
                
                # è®°å½•æ¨é€å†å²
                for channel, success in results.items():
                    db.add_push_history(
                        channel=channel,
                        source="combined",
                        title=message.title,
                        item_count=len(all_items),
                        status="success" if success else "failed"
                    )
                    if success:
                        total_pushed += 1
                
                logger.info(f"åˆå¹¶æ¨é€ç»“æœ: {results}")
                
                # æ ‡è®°æ‰€æœ‰å·²æ¨é€
                for _, source, _, new_items in all_updates:
                    rss_fetcher.mark_as_pushed(source, new_items)

            self._last_run_result = {
                "success": True,
                "sources_count": len(hot_lists),
                "new_items": total_new,
                "pushed_count": total_pushed
            }
            logger.info(f"æŠ“å–å®Œæˆï¼Œå…± {len(hot_lists)} ä¸ªæºï¼Œ{total_new} æ¡æ–°å†…å®¹")

        except Exception as e:
            logger.error(f"æŠ“å–ä»»åŠ¡å¤±è´¥: {e}")
            self._last_run_result = {
                "success": False,
                "error": str(e)
            }

    def _apply_rules(self, items: List[HotItem], source: str, rules: list) -> List[HotItem]:
        """åº”ç”¨æ¨é€è§„åˆ™è¿‡æ»¤"""
        if not rules:
            return items

        filtered = items
        now = datetime.now()

        for rule in rules:
            rule_type = rule["rule_type"]
            config = rule["rule_config"]

            if rule_type == "keyword_include":
                # åªä¿ç•™åŒ…å«å…³é”®è¯çš„
                keywords = config.get("keywords", [])
                filtered = [
                    item for item in filtered
                    if any(kw.lower() in item.title.lower() for kw in keywords)
                ]

            elif rule_type == "keyword_exclude":
                # æ’é™¤åŒ…å«å…³é”®è¯çš„
                keywords = config.get("keywords", [])
                filtered = [
                    item for item in filtered
                    if not any(kw.lower() in item.title.lower() for kw in keywords)
                ]

            elif rule_type == "time_range":
                # æ—¶é—´æ®µé™åˆ¶
                start_hour = config.get("start_hour", 0)
                end_hour = config.get("end_hour", 23)
                weekdays = config.get("weekdays", [])

                current_hour = now.hour
                current_weekday = now.isoweekday()  # 1=Monday, 7=Sunday

                # æ£€æŸ¥æ˜ŸæœŸ
                if weekdays and current_weekday not in weekdays:
                    filtered = []
                    continue

                # æ£€æŸ¥æ—¶é—´
                if start_hour <= end_hour:
                    if not (start_hour <= current_hour <= end_hour):
                        filtered = []
                else:
                    # è·¨è¶Šåˆå¤œçš„æƒ…å†µ
                    if not (current_hour >= start_hour or current_hour <= end_hour):
                        filtered = []

            elif rule_type == "source_filter":
                # æ¥æºè¿‡æ»¤
                sources_list = config.get("sources", [])
                mode = config.get("mode", "include")

                if mode == "include" and source not in sources_list:
                    filtered = []
                elif mode == "exclude" and source in sources_list:
                    filtered = []

        return filtered

    def start(self):
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
        interval = self._get_interval()

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¯ç”¨
        enabled_setting = db.get_setting("scheduler_enabled")
        if enabled_setting == "0":
            self._is_paused = True

        # æ·»åŠ å®šæ—¶ä»»åŠ¡
        self.scheduler.add_job(
            self._fetch_and_push_job,
            trigger=IntervalTrigger(minutes=interval),
            id="fetch_and_push",
            name="æŠ“å–çƒ­æ¦œå¹¶æ¨é€",
            replace_existing=True
        )

        # å¯åŠ¨è°ƒåº¦å™¨
        self.scheduler.start()

        if self._is_paused:
            self.pause()
            logger.info(f"å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ä½†æš‚åœä¸­ï¼Œé—´éš” {interval} åˆ†é’Ÿ")
        else:
            logger.info(f"å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ¯ {interval} åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡")

        # åˆå§‹åŒ–æ‘˜è¦ä»»åŠ¡
        digest_config = self.get_digest_config()
        if digest_config.get("enabled"):
            self._update_digest_job(digest_config)

    def stop(self):
        """åœæ­¢å®šæ—¶ä»»åŠ¡"""
        if self.scheduler.running:
            self.scheduler.shutdown()
            logger.info("å®šæ—¶ä»»åŠ¡å·²åœæ­¢")


# å…¨å±€å®ä¾‹
scheduler_service = SchedulerService()


# å…¼å®¹æ—§æ¥å£
def start_scheduler():
    scheduler_service.start()


def stop_scheduler():
    scheduler_service.stop()


async def run_once():
    await scheduler_service.trigger_fetch()
